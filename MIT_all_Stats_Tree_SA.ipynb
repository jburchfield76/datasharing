{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvlKZXFLJUn6TZssjxEizw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jburchfield76/datasharing/blob/master/MIT_all_Stats_Tree_SA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZOYnHPCoYyq",
        "outputId": "cc2599e1-f640-42b0-c1d1-72ee464867e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (462, 10)\n",
            "\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 462 entries, 0 to 461\n",
            "Data columns (total 10 columns):\n",
            " #   Column     Non-Null Count  Dtype   \n",
            "---  ------     --------------  -----   \n",
            " 0   chd        462 non-null    category\n",
            " 1   sbp        462 non-null    float64 \n",
            " 2   tobacco    462 non-null    float64 \n",
            " 3   ldl        462 non-null    float64 \n",
            " 4   adiposity  462 non-null    float64 \n",
            " 5   famhist    462 non-null    category\n",
            " 6   typea      462 non-null    float64 \n",
            " 7   obesity    462 non-null    float64 \n",
            " 8   alcohol    462 non-null    float64 \n",
            " 9   age        462 non-null    float64 \n",
            "dtypes: category(2), float64(8)\n",
            "memory usage: 30.1 KB\n",
            "None\n",
            "\n",
            "Tree Model Summary:\n",
            "Number of nodes: 193\n",
            "Maximum depth: 14\n",
            "Number of leaves: 97\n",
            "\n",
            "Cross-validation results:\n",
            "Optimal tree depth: 3\n",
            "Minimum misclassification error: 0.307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3376062710.py:90: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"r-\" (-> color='r'). The keyword argument will take precedence.\n",
            "  plt.plot(size, score, 'r-', linewidth=2, color='#d62728')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pruned Tree Summary:\n",
            "Number of nodes: 15\n",
            "Maximum depth: 3\n",
            "Number of leaves: 8\n",
            "\n",
            "Feature Importances:\n",
            "     feature  importance\n",
            "8        age    0.542465\n",
            "1    tobacco    0.147369\n",
            "4    famhist    0.128857\n",
            "5      typea    0.120395\n",
            "2        ldl    0.060914\n",
            "0        sbp    0.000000\n",
            "3  adiposity    0.000000\n",
            "6    obesity    0.000000\n",
            "7    alcohol    0.000000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      No CHD       0.89      0.66      0.75        61\n",
            "         CHD       0.56      0.84      0.68        32\n",
            "\n",
            "    accuracy                           0.72        93\n",
            "   macro avg       0.73      0.75      0.71        93\n",
            "weighted avg       0.78      0.72      0.73        93\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[40 21]\n",
            " [ 5 27]]\n"
          ]
        }
      ],
      "source": [
        "#converted tree.r from MIT-all of stats to python using grok\n",
        "#converted dat file to csv, BUT DID NOT NEED TO, SEE BELOW pd.read_csv(\"sa.data\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Read data (skip first row, comma-separated)\n",
        "# Note: You'll need to adjust the file path to your actual file location\n",
        "data = pd.read_csv(\"sa.data\", skiprows=1, header=None)\n",
        "\n",
        "# Create matrix with 11 columns, by row\n",
        "X = data.values.reshape(-1, 11)\n",
        "\n",
        "# Extract chd (11th column)\n",
        "chd = X[:, 10]  # Python uses 0-based indexing, so column 11 is index 10\n",
        "\n",
        "# Remove first and 11th columns\n",
        "X = np.delete(X, [0, 10], axis=1)\n",
        "\n",
        "# Define feature names\n",
        "names = [\"sbp\", \"tobacco\", \"ldl\", \"adiposity\", \"famhist\", \"typea\", \"obesity\", \"alcohol\", \"age\"]\n",
        "\n",
        "# Create DataFrame with feature columns\n",
        "feature_df = pd.DataFrame(X, columns=names)\n",
        "\n",
        "# Convert famhist to categorical\n",
        "feature_df['famhist'] = feature_df['famhist'].astype('category')\n",
        "\n",
        "# Create target variable as categorical\n",
        "chd_series = pd.Series(chd, name='chd')\n",
        "chd_series = chd_series.astype('category')\n",
        "\n",
        "# Create complete dataset\n",
        "d = pd.concat([chd_series, feature_df], axis=1)\n",
        "\n",
        "print(\"Dataset shape:\", d.shape)\n",
        "print(\"\\nDataset info:\")\n",
        "print(d.info())\n",
        "\n",
        "# Decision Tree (equivalent to R's tree package)\n",
        "# Fit the tree model\n",
        "tree_model = DecisionTreeClassifier(random_state=42, criterion='gini')  # gini is similar to misclass\n",
        "tree_model.fit(d.drop('chd', axis=1), d['chd'])\n",
        "\n",
        "print(\"\\nTree Model Summary:\")\n",
        "print(f\"Number of nodes: {tree_model.tree_.node_count}\")\n",
        "print(f\"Maximum depth: {tree_model.tree_.max_depth}\")\n",
        "print(f\"Number of leaves: {np.sum(tree_model.tree_.children_left == -1)}\")\n",
        "\n",
        "# Plot 1: Tree visualization\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "plot_tree(tree_model,\n",
        "          feature_names=names,\n",
        "          class_names=['No CHD', 'CHD'],\n",
        "          filled=True,\n",
        "          ax=ax,\n",
        "          fontsize=8)\n",
        "plt.title('South Africa CHD Decision Tree - Full Tree')\n",
        "plt.savefig('south.africa.tree.plot1.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Cross-validation to find optimal tree size\n",
        "# We'll use cross-validation to find the best max_depth (similar to pruning)\n",
        "depths = range(1, 15)\n",
        "cv_scores = []\n",
        "\n",
        "for depth in depths:\n",
        "    model = DecisionTreeClassifier(max_depth=depth, random_state=42, criterion='gini')\n",
        "    scores = cross_val_score(model, d.drop('chd', axis=1), d['chd'],\n",
        "                           cv=10, scoring='accuracy')\n",
        "    cv_scores.append(1 - np.mean(scores))  # Convert to misclassification error\n",
        "\n",
        "cv_scores = np.array(cv_scores)\n",
        "size = depths\n",
        "score = cv_scores\n",
        "\n",
        "# Find optimal size (depth)\n",
        "min_score_idx = np.argmin(score)\n",
        "optimal_k = size[min_score_idx]\n",
        "\n",
        "print(f\"\\nCross-validation results:\")\n",
        "print(f\"Optimal tree depth: {optimal_k}\")\n",
        "print(f\"Minimum misclassification error: {score[min_score_idx]:.3f}\")\n",
        "\n",
        "# Plot 2: CV error vs tree size\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(size, score, 'r-', linewidth=2, color='#d62728')\n",
        "plt.scatter(optimal_k, score[min_score_idx], color='red', s=100, zorder=5)\n",
        "plt.xlabel('Tree Size (Depth)')\n",
        "plt.ylabel('Cross-Validation Misclassification Error')\n",
        "plt.title('Cross-Validation Error vs Tree Size')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('south.africa.tree.plot2.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Prune the tree to optimal size\n",
        "pruned_model = DecisionTreeClassifier(max_depth=optimal_k, random_state=42, criterion='gini')\n",
        "pruned_model.fit(d.drop('chd', axis=1), d['chd'])\n",
        "\n",
        "print(f\"\\nPruned Tree Summary:\")\n",
        "print(f\"Number of nodes: {pruned_model.tree_.node_count}\")\n",
        "print(f\"Maximum depth: {pruned_model.tree_.max_depth}\")\n",
        "print(f\"Number of leaves: {np.sum(pruned_model.tree_.children_left == -1)}\")\n",
        "\n",
        "# Plot 3: Pruned tree visualization\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "plot_tree(pruned_model,\n",
        "          feature_names=names,\n",
        "          class_names=['No CHD', 'CHD'],\n",
        "          filled=True,\n",
        "          ax=ax,\n",
        "          fontsize=8)\n",
        "plt.title(f'South Africa CHD Decision Tree - Pruned (Depth={optimal_k})')\n",
        "plt.savefig('south.africa.tree.plot3.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': names,\n",
        "    'importance': pruned_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(feature_importance)\n",
        "\n",
        "# Optional: Print confusion matrix and accuracy for the pruned model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data for evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    d.drop('chd', axis=1), d['chd'], test_size=0.2, random_state=42, stratify=d['chd']\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "pruned_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pruned_model.predict(X_test)\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No CHD', 'CHD']))\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ]
    }
  ]
}